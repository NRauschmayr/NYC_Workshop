{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Recurrent Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*SKGAqkVVzT6co-sZ29ze-g.png)\n",
    "*<sub><sub>Taken from https://medium.com/deeplearningbrasilia/deep-learning-recurrent-neural-networks-f9482a24d010</sub></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "![](http://karpathy.github.io/assets/rnn/diags.jpeg)\n",
    "*Taken from http://karpathy.github.io/2015/05/21/rnn-effectiveness/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Networks\n",
    "- exhibit temporal dynamic behavior\n",
    "- internal state \n",
    "- process sequences of inputs\n",
    "![](https://cdn-images-1.medium.com/max/800/1*AQ52bwW55GsJt6HTxPDuMA.gif)\n",
    "*<sub><sub>Taken from https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Networks\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*o-Cq5U8-tfa1_ve2Pf3nfg.gif)\n",
    "*<sub><sub>Taken from https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Networks\n",
    "- concatenate input vector and hidden state\n",
    "- output is the new hidden state\n",
    "![](https://cdn-images-1.medium.com/max/800/1*WMnFSJHzOloFlJHU6fVN-g.gif)\n",
    "*<sub><sub>Taken from https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Recurrent Neural Networks\n",
    "- *tanh* squishes value between -1 and +1\n",
    "![](https://cdn-images-1.medium.com/max/800/1*iRlEg1GBKRzGTre5aOQUCg.gif)\n",
    "*<sub><sub>Taken from https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Character Level Languag Models\n",
    "![](http://karpathy.github.io/assets/rnn/charseq.jpeg)\n",
    "*Taken from http://karpathy.github.io/2015/05/21/rnn-effectiveness/* "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "\n",
    "<div align=\"center\"> What could be the problems of RNNs? </div>   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem with RNNs\n",
    "- vanshing gradients\n",
    "- short-term memory\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*LgbEFcGiUpseZ--M7wuZhg.gif)\n",
    "*<sub><sub>Taken from https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Short Term Memory (LSTM)\n",
    "- Input Gate\n",
    "- Output Gate\n",
    "- Forget Gate\n",
    "\n",
    "<img src=\"https://colah.github.io/posts/2015-08-Understanding-LSTMs/img/LSTM3-chain.png\" width=\"1000\" height=\"400\"/>\n",
    "\n",
    "\n",
    "*<sup><sub>Taken from https://colah.github.io/posts/2015-08-Understanding-LSTMs/</sup></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Short Term Memory (LSTM)\n",
    "- input and hidden state are concatenaned\n",
    "- the closer $f_t$ is to 0 the more likely it will be forgotten\n",
    "- $f_t$ is added to the cell state $c_t$\n",
    "![](https://cdn-images-1.medium.com/max/800/1*GjehOa513_BgpDDP6Vkw2Q.gif)\n",
    "*<sub><sub>Taken from https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Short Term Memory (LSTM)\n",
    "\n",
    "- vector of new candidate values that could be added to the cell state \n",
    "- input gate decides what is relevant to add from current step\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/800/1*TTmYy7Sy8uUXxUXfzmoKbA.gif)\n",
    "*<sub><sub>Taken from https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Short Term Memory (LSTM)\n",
    "- cell state is multiplied with forget vector\n",
    "- output of input gate is added to cell state\n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*S0rXIeO_VoUVOyrYHckUWg.gif)\n",
    "*<sub><sub>Taken from https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Long Short Term Memory (LSTM)\n",
    "\n",
    "- previous hidden state and current input goes into *sigmoid*\n",
    "- new cell state goes into *tanh*\n",
    "- new hidden state is the product of the outputs \n",
    "\n",
    "![](https://cdn-images-1.medium.com/max/1600/1*VOXRGhOShoWWks6ouoDN3Q.gif)\n",
    "*<sub><sub>Taken from https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21</sub></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs and its applications\n",
    "\n",
    "#### Sentiment Analysis\n",
    "\n",
    "![](https://openai.com/content/images/2017/04/low_res_maybe_faster.gif)\n",
    "*Taken from https://openai.com/blog/unsupervised-sentiment-neuron/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### LSTMs and its applications \n",
    "\n",
    "- Speech recognition\n",
    "- Text generation  \n",
    "- Music Generation\n",
    "- Machine translation\n",
    "- Image Captioning\n",
    "- OCR optical character recognition\n",
    "- Sequence tagging\n",
    "- Video recognition\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Bidirectional RNNs\n",
    "\n",
    "![](http://colah.github.io/posts/2015-09-NN-Types-FP/img/RNN-bidirectional.png)\n",
    "*<sup><sub>Taken from https://towardsdatascience.com/understanding-bidirectional-rnn-in-pytorch-5bd25a5dd66 </sup></sub>*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### BiLSTMs and its applications\n",
    "- Deep contextualized word representations https://arxiv.org/pdf/1802.05365.pdf\n",
    "- Named Entity Recognition with Bidirectional LSTM-CNNs https://arxiv.org/pdf/1511.08308v5.pdf\n",
    "- End-to-end Sequence Labeling via Bi-directional LSTM-CNNs-CRF https://arxiv.org/pdf/1603.01354v5.pdf\n",
    "- RWTH ASR Systems for LibriSpeech: Hybrid vs Attention - w/o Data Augmentatio https://arxiv.org/abs/1905.03072"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "For more details on RNN, LSTM, GRU, check out the following great tutorials:\n",
    "- https://towardsdatascience.com/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21 \n",
    "- https://colah.github.io/posts/2015-08-Understanding-LSTMs/"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
